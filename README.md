# AI Text Detection - NLP Final Project

**Course:** SEA 820 NLP Final Project
**Student:** Inderpreet Singh Parmar
**Goal:** Compare different methods to detect whether text was written by AI or by humans.

**VIDEO PRESENTATION LINK:-**  https://youtu.be/MChtQv5v2mw
## What I Did

I tested four different approaches to see which one works best:

1. **TF-IDF + Naive Bayes** – A basic machine learning method.
2. **TF-IDF + Logistic Regression** – Another classic ML approach for text classification.
3. **DistilBERT Fine-tuning** – A modern transformer-based method.
4. **DistilBERT + LoRA** – A more efficient fine-tuning method that uses fewer trainable parameters.


## How to Run This

### Requirements

* Python 3.8 or newer
* Google Colab (recommended for GPU access)
* A GPU runtime for training transformer models

### Install Dependencies in Colab

```bash
!pip install transformers datasets accelerate peft
!pip install scikit-learn nltk pandas matplotlib seaborn
```

### Steps

1. Upload the notebook to Google Colab.
2. Switch the runtime to **GPU** (Runtime → Change runtime type → GPU).
3. Run all cells in order from top to bottom.
   *(Full training can take about 3–4 hours.)*

## Project Files

```
ai-text-detection-nlp-project/
├── README.md                 # This file
├── notebook
├── requirements.txt          # Package versions
└── .gitignore
```

*Note: The trained models are not included because of file size limits, but they can be generated by running the notebook.*

## Approach Summary

* **Data:** Dataset of AI-generated vs. human-written text.
* **Preprocessing:** Different preprocessing pipelines for classical ML and transformer models.
* **Training:** Fine-tuned DistilBERT and applied LoRA for parameter-efficient training.
* **Evaluation:** Used accuracy, precision, recall, and F1-score to measure performance.
* **Analysis:** Compared model performance and noted the types of inputs that caused misclassifications.

## What I Learned

* Transformer models (DistilBERT) generally outperformed the classical ML models, but they required more training time.
* LoRA provided a good balance—close performance to full fine-tuning with faster training and lower memory usage.
* Very short texts or unusual writing styles were more challenging for all models.
* Classical models are quick to train and easy to deploy but can’t match the deep contextual understanding of transformers.

## Challenges

* Long training times without a good GPU.
* Occasional library version conflicts when running locally.
* LoRA required matching library versions to work properly.

## References

### Course Materials

1. SEA 820 NLP Course – Week 7: Fine-tuning DistilBERT for Text Classification (Trainer API)
2. SEA 820 NLP Course – Week 10: Parameter-Efficient Fine-Tuning (LoRA/PEFT)
3. SEA 820 NLP Course – Weeks 1–6: TF-IDF, Naive Bayes, Logistic Regression

### Official Documentation

4. Hugging Face Transformers – Trainer API: [https://huggingface.co/docs/transformers/main\_classes/trainer](https://huggingface.co/docs/transformers/main_classes/trainer)
5. Hugging Face Datasets: [https://huggingface.co/docs/datasets](https://huggingface.co/docs/datasets)
6. Hugging Face AutoTokenizer: [https://huggingface.co/docs/transformers/main\_classes/tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)
7. Hugging Face AutoModelForSequenceClassification: [https://huggingface.co/docs/transformers/model\_doc/auto](https://huggingface.co/docs/transformers/model_doc/auto)
8. Hugging Face PEFT: [https://huggingface.co/docs/peft](https://huggingface.co/docs/peft)
9. Scikit-learn: [https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)
10. NLTK: [https://www.nltk.org/](https://www.nltk.org/)
11. Pandas: [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/)
12. NumPy: [https://numpy.org/doc/](https://numpy.org/doc/)
13. Matplotlib: [https://matplotlib.org/stable/contents.html](https://matplotlib.org/stable/contents.html)
14. Seaborn: [https://seaborn.pydata.org/](https://seaborn.pydata.org/)

### Research Papers

15. Sanh, Victor, et al. "DistilBERT, a distilled version of BERT." *arXiv:1910.01108* (2019). [https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)
16. Hu, Edward J., et al. "LoRA: Low-Rank Adaptation of Large Language Models." *arXiv:2106.09685* (2021). [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)
17. Devlin, Jacob, et al. "BERT: Pre-training of Deep Bidirectional Transformers." *arXiv:1810.04805* (2018). [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
18. Vaswani, Ashish, et al. "Attention is All You Need." *NeurIPS* 30 (2017). [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

### Pre-trained Models

19. DistilBERT-base-uncased – [https://huggingface.co/distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased)

### Development Environment

20. Google Colab – [https://colab.research.google.com/](https://colab.research.google.com/)
21. GitHub – [https://github.com/](https://github.com/)

### Technical References

22. PyTorch – [https://pytorch.org/docs/stable/index.html](https://pytorch.org/docs/stable/index.html)
23. Accelerate – [https://huggingface.co/docs/accelerate](https://huggingface.co/docs/accelerate)
24. Hugging Face Tokenizers – [https://huggingface.co/docs/tokenizers](https://huggingface.co/docs/tokenizers)

### Evaluation Methodology

25. Powers, David M. "Evaluation: from precision, recall and F-measure to ROC." *arXiv:2010.16061* (2020).
26. Sokolova, Marina, and Guy Lapalme. "A systematic analysis of performance measures." *Information Processing & Management* 45.4 (2009): 427–437.

---

This project was completed as part of the SEA 820 NLP course. All code was developed following the course materials and my own experiments/research.
